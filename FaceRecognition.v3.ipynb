{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of FaceRecognition.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "file_extension": ".py",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "mimetype": "text/x-python",
    "name": "python",
    "npconvert_exporter": "python",
    "pygments_lexer": "ipython3",
    "version": 3,
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hg0SdkJXWX4M",
        "outputId": "7c9b605c-60c4-435a-9fa8-83a36c0373d7"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mfFw_HG-KvAB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "153c67e2-a745-4e19-d085-ce347c6f7b5a"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "#Load already prepared CK+\n",
        "def load_CKplus_full():\n",
        "  x = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/SIT723/inputs.csv\",header=None)\n",
        "  y = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/SIT723/labels.csv\",header=None)\n",
        "  x, y = shuffle(x, y,random_state = 2021)\n",
        "\n",
        "  x = np.array(x)\n",
        "  x = x.reshape(len(x),48,48,1)\n",
        "  y = np.array(y)\n",
        "  return x, y\n",
        "\n",
        "def load_CKplus_split():\n",
        "  x = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/SIT723/inputs.csv\",header=None)\n",
        "  y = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/SIT723/labels.csv\",header=None)\n",
        "  x, y = shuffle(x, y,random_state = 2021)\n",
        "  x_train, x_test, y_train, y_test = train_test_split(x, y,test_size=147, random_state=2021)\n",
        "  x_train, x_val, y_train, y_val = train_test_split(x_train, y_train,test_size=147, random_state=2021)\n",
        "\n",
        "  x_train = np.array(x_train)\n",
        "  x_train = x_train.reshape(len(x_train),48,48,1)\n",
        "  y_train = np.array(y_train)\n",
        "  x_test = np.array(x_test)\n",
        "  x_test = x_test.reshape(len(x_test),48,48,1)\n",
        "  y_test = np.array(y_test)\n",
        "  x_val = np.array(x_val)\n",
        "  x_val = x_val.reshape(len(x_val),48,48,1)\n",
        "  y_val = np.array(y_val)\n",
        "  return x_train, y_train, x_test, y_test, x_val, y_val\n",
        "\n",
        "#Load already prepared FER2013\n",
        "def load_FER2013():\n",
        "  x_train = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/SIT723/x_train.csv\",header=None,)\n",
        "  y_train = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/SIT723/y_train.csv\",header=None)\n",
        "  x_test = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/SIT723/x_test.csv\",header=None)\n",
        "  y_test = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/SIT723/y_test.csv\",header=None)\n",
        "  x_val = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/SIT723/x_val.csv\",header=None)\n",
        "  y_val = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/SIT723/y_val.csv\",header=None)\n",
        "\n",
        "  x_train = np.array(x_train)\n",
        "  x_train = x_train.reshape(len(x_train),48,48,1)\n",
        "  y_train = np.array(y_train)\n",
        "  x_test = np.array(x_test)\n",
        "  x_test = x_test.reshape(len(x_test),48,48,1)\n",
        "  y_test = np.array(y_test)\n",
        "  x_val = np.array(x_val)\n",
        "  x_val = x_val.reshape(len(x_val),48,48,1)\n",
        "  y_val = np.array(y_val)\n",
        "  return x_train, y_train, x_test, y_test, x_val, y_val\n",
        "\n",
        "\n",
        "x_train, y_train, x_test, y_test, x_val, y_val = load_FER2013()\n",
        "\n",
        "x_train.shape, x_test.shape, x_val.shape\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((28709, 48, 48, 1), (3589, 48, 48, 1), (3589, 48, 48, 1))"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E1RMfxmyy4qI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0dc20ee2-1d0f-4ea9-d8db-f689d0626f32"
      },
      "source": [
        "#face recognition packages\n",
        "!pip install opencv-python\n",
        "!pip install face_recognition"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (4.1.2.30)\n",
            "Requirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from opencv-python) (1.19.5)\n",
            "Collecting face_recognition\n",
            "  Downloading face_recognition-1.3.0-py2.py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: dlib>=19.7 in /usr/local/lib/python3.7/dist-packages (from face_recognition) (19.18.0)\n",
            "Collecting face-recognition-models>=0.3.0\n",
            "  Downloading face_recognition_models-0.3.0.tar.gz (100.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 100.1 MB 10 kB/s \n",
            "\u001b[?25hRequirement already satisfied: Click>=6.0 in /usr/local/lib/python3.7/dist-packages (from face_recognition) (7.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from face_recognition) (1.19.5)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from face_recognition) (7.1.2)\n",
            "Building wheels for collected packages: face-recognition-models\n",
            "  Building wheel for face-recognition-models (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for face-recognition-models: filename=face_recognition_models-0.3.0-py2.py3-none-any.whl size=100566185 sha256=2ee55d43d7cfb4fd121389f3c734b3c9e102c7547e8ceec30c8b1dc7b319d3b7\n",
            "  Stored in directory: /root/.cache/pip/wheels/d6/81/3c/884bcd5e1c120ff548d57c2ecc9ebf3281c9a6f7c0e7e7947a\n",
            "Successfully built face-recognition-models\n",
            "Installing collected packages: face-recognition-models, face-recognition\n",
            "Successfully installed face-recognition-1.3.0 face-recognition-models-0.3.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WCCz8HFmm--d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "afdb5872-4859-4839-c6bc-6ec3982b5683"
      },
      "source": [
        "#face detection, crop, sharpen, emboss using both face_recognition and cv2 packages\n",
        "import face_recognition\n",
        "from skimage.feature import local_binary_pattern\n",
        "import cv2\n",
        "import math\n",
        "from google.colab.patches import cv2_imshow\n",
        "from PIL import Image, ImageEnhance\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def transform(img):\n",
        "  if len(img.shape)>2:\n",
        "    img = img.reshape(48,48)\n",
        "  #gamma\n",
        "  mid = 0.5 \n",
        "  mean = np.mean(img)\n",
        "  gamma = math.log(mid*255)/math.log(mean)\n",
        "  img = np.power(img, gamma).clip(0,255).astype(np.uint8)\n",
        "\n",
        "  img = Image.fromarray(img)\n",
        "  #filter = ImageEnhance.Contrast(img)# add contrast\n",
        "  #img = filter.enhance(2)\n",
        "\n",
        "  #img = img.filter(ImageEnhance.ImageFilter.SMOOTH)\n",
        "  #img = img.filter(ImageEnhance.ImageFilter.EMBOSS)#emboss\n",
        "  #img = img.filter(ImageEnhance.ImageFilter.EDGE_ENHANCE)\n",
        "  \n",
        "  img = local_binary_pattern(img, 30, 1)\n",
        "  img = np.array(img)\n",
        "  return img\n",
        "def pre_process(input, target, output_height=48, output_width=48):\n",
        "  face_cascade_name = cv2.data.haarcascades + 'haarcascade_frontalface_alt.xml'\n",
        "  face_cascade = cv2.CascadeClassifier()\n",
        "  if not face_cascade.load(cv2.samples.findFile(face_cascade_name)):\n",
        "    print(\"Error loading xml file\")\n",
        "    exit(0)\n",
        "\n",
        "  x_temp = []\n",
        "  y_temp = []\n",
        "  #plt.figure(figsize=[8,8])\n",
        "  for i in range(len(input)):\n",
        "    img = np.array(input[i],'uint8')\n",
        "    face_detect_1 = face_recognition.face_locations(img) #Using face_recoginition\n",
        "    face_detect_2 = face_cascade.detectMultiScale(img,1.1, 1) #Using cv2\n",
        "    if (len(face_detect_1)>0):\n",
        "      (y, w, h, x) = face_detect_1[0]\n",
        "      #img = img[y:y + h, x:x + w]\n",
        "      #img = cv2.resize(img,(output_height, output_width))\n",
        "      \n",
        "      img = transform(img)\n",
        "      img = img.reshape(output_height,output_width,1)\n",
        "      x_temp.append(img)\n",
        "      y_temp.append(target[i])\n",
        "\n",
        "    elif (len(face_detect_2)>0):\n",
        "      (x, y, w, h) = face_detect_2[0]\n",
        "      #img = img[y:y + h, x:x + w]\n",
        "      #img = cv2.resize(img, (output_height, output_width))\n",
        "      \n",
        "      img = transform(img)\n",
        "      img = img.reshape(output_height,output_width,1)\n",
        "      x_temp.append(img)\n",
        "      y_temp.append(target[i])\n",
        "\n",
        "    #cv2.rectangle(img, (x, y), (x+w, y+h), (0,0,0), 1)\n",
        "    #print(i, x, y, w, h)\n",
        "\n",
        "  x_temp = np.array(x_temp)\n",
        "  y_temp = np.array(y_temp)\n",
        "  return x_temp, y_temp\n",
        "\n",
        "x_train, y_train = pre_process(x_train, y_train)\n",
        "x_test, y_test = pre_process(x_test, y_test)\n",
        "x_val, y_val = pre_process(x_val, y_val)\n",
        "\n",
        "#For conveniency when training\n",
        "trainx = x_train.copy()\n",
        "trainy = y_train.copy()\n",
        "testx = x_test.copy()\n",
        "testy = y_test.copy()\n",
        "valx = x_val.copy()\n",
        "valy = y_val.copy()\n",
        "\n",
        "x_train.shape, y_train.shape"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((22279, 48, 48, 1), (22279, 1))"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9BbBFxf5gAHU"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "# Displaying 25 images from training dataset in 5x5 matrix\n",
        "emotion={0:'Angry', 1:'Disgust', 2:'Fear', 3:'Happy', 4:'Sad', 5:'Surprise', 6:'Neutral'} #for FER2013\n",
        "#emotion = {0:'anger',1:'contempt',2:'disgust',3:'fear',4:'happy',5:'sadness',6:'surprise'} #for CK+\n",
        "def plot_image(input_images,labels):\n",
        "  print(\"Display 50 images from the train set in the form of 5x5 matrix with their respective labels:\")\n",
        "  plt.figure(figsize=[16,8])\n",
        "  for i in range(50):\n",
        "    plt.subplot(5,10,1+i)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.grid(False)\n",
        "    plt.imshow(input_images[i].reshape([48,48]), cmap='gray')\n",
        "    plt.title(emotion[int(labels[i])])\n",
        "  plt.show()\n",
        "\n",
        "plot_image(x_train,y_train)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yqGgUcKtWOJ8"
      },
      "source": [
        "#CBAM \n",
        "\n",
        "from __future__ import print_function\n",
        "import keras\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from keras.layers import Dense, Conv2D, BatchNormalization, Activation\n",
        "from keras.layers import AveragePooling2D, Input, Flatten\n",
        "from tensorflow.keras.optimizers import Adam, RMSprop, SGD\n",
        "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.regularizers import l2\n",
        "from keras import backend as K\n",
        "from keras.models import Model\n",
        "from keras.datasets import cifar10\n",
        "import resnet_v1, mobilenets\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "# Training parameters\n",
        "batch_size = 128\n",
        "epochs = 60\n",
        "data_augmentation = True\n",
        "num_classes = 7\n",
        "subtract_pixel_mean = False  # Subtracting pixel mean improves accuracy\n",
        "attention_module = 'cbam_block'\n",
        "base_model = 'MobileNet'\n",
        "# Choose what attention_module to use: cbam_block / se_block / None\n",
        "model_type = base_model if attention_module==None else base_model+' '+attention_module\n",
        "\n",
        "\n",
        "# Load dataset.\n",
        "\n",
        "x_train = trainx.copy()\n",
        "y_train = trainy.copy()\n",
        "x_test = testx.copy()\n",
        "y_test = testy.copy()\n",
        "x_val = valx.copy()\n",
        "y_val = valy.copy()\n",
        "\n",
        "x_train = np.concatenate((x_train, x_test), axis=0)\n",
        "y_train = np.concatenate((y_train, y_test), axis=0)\n",
        "x_train = np.concatenate((x_train, x_val), axis=0)\n",
        "y_train = np.concatenate((y_train, y_val), axis=0)\n",
        "\n",
        "x_train, x_test, y_train, y_test  = train_test_split(x_train, y_train, test_size= 0.1,shuffle=True, random_state=2021)\n",
        "x_train, x_val, y_train, y_val  = train_test_split(x_train, y_train, test_size= 0.2, shuffle=True, random_state=2021)\n",
        "\n",
        "# Input image dimensions.\n",
        "input_shape = x_train.shape[1:]\n",
        "\n",
        "# Normalize data.\n",
        "#x_train = x_train.astype('float32') / 255\n",
        "#x_test = x_test.astype('float32') / 255\n",
        "#x_val = x_test.astype('float32') / 255\n",
        "\n",
        "# If subtract pixel mean is enabled\n",
        "if subtract_pixel_mean:\n",
        "    x_train_mean = np.mean(x_train, axis=0)\n",
        "    x_train -= x_train_mean\n",
        "    x_test -= x_train_mean\n",
        "\n",
        "\n",
        "# Convert class vectors to binary class matrices.\n",
        "y_train = to_categorical(y_train, num_classes)\n",
        "y_test = to_categorical(y_test, num_classes)\n",
        "y_val = to_categorical(y_val, num_classes)\n",
        "\n",
        "\n",
        "depth = 20 # For ResNet, specify the depth (e.g. ResNet50: depth=50, ResNet101: Depth=101)\n",
        "model = resnet_v1.resnet_v1(input_shape=input_shape, depth=depth,num_classes=7, attention_module='cbam_block')\n",
        "#model = mobilenets.MobileNet(input_shape=input_shape, classes=num_classes, attention_module='cbam_block')\n",
        "#model.compile(loss='categorical_crossentropy',optimizer=Adam(lr=lr_schedule(0)),metrics=['accuracy'])\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=Adam(), #SGD(decay=2e-5, momentum=0.9, nesterov=True),\n",
        "              metrics=['accuracy'])\n",
        "#model.summary()\n",
        "\n",
        "# Prepare model model saving directory.\n",
        "save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
        "model_name = 'cifar10_%s_model.{epoch:03d}.h5' % model_type\n",
        "if not os.path.isdir(save_dir):\n",
        "    os.makedirs(save_dir)\n",
        "filepath = os.path.join(save_dir, model_name)\n",
        "\n",
        "# Prepare callbacks for model saving and for learning rate adjustment.\n",
        "checkpoint = ModelCheckpoint(filepath=filepath,\n",
        "                             monitor='val_acc',\n",
        "                             verbose=1,\n",
        "                             save_best_only=True)\n",
        "def lr_schedule(epoch):\n",
        "    lr = 0.01\n",
        "    if epoch >= (epochs*0.5):\n",
        "      lr = 0.0001\n",
        "    elif epoch >= (epochs*0.2):\n",
        "      lr=0.001\n",
        "    print('Learning rate: ', lr)\n",
        "    return lr\n",
        "\n",
        "lr_scheduler = LearningRateScheduler(lr_schedule)\n",
        "lr_reducer = ReduceLROnPlateau(factor=np.sqrt(0.1),\n",
        "                               cooldown=0,\n",
        "                               patience=5,\n",
        "                               min_lr=0.5e-6)\n",
        "callbacks = [checkpoint, lr_reducer, lr_scheduler]\n",
        "\n",
        "# Run training, with or without data augmentation.\n",
        "if not data_augmentation:\n",
        "    print('Not using data augmentation.')\n",
        "    #history = model.fit(x_train, y_train,batch_size=batch_size, epochs=epochs,validation_data=(x_val, y_val), shuffle=True,callbacks=callbacks)\n",
        "    history = model.fit(x_train, y_train,batch_size=batch_size, epochs=epochs,validation_data=(x_val, y_val),shuffle=True, callbacks=callbacks)\n",
        "else:\n",
        "    print('Using real-time data augmentation.')\n",
        "    # This will do preprocessing and realtime data augmentation:\n",
        "    datagen = ImageDataGenerator(\n",
        "        # set input mean to 0 over the dataset\n",
        "        featurewise_center=False,\n",
        "        # set each sample mean to 0\n",
        "        samplewise_center=False,\n",
        "        # divide inputs by std of dataset\n",
        "        featurewise_std_normalization=False,\n",
        "        # divide each input by its std\n",
        "        samplewise_std_normalization=False,\n",
        "        # apply ZCA whitening\n",
        "        zca_whitening=False,\n",
        "        # epsilon for ZCA whitening\n",
        "        zca_epsilon=1e-06,\n",
        "        # randomly rotate images in the range (deg 0 to 180)\n",
        "        rotation_range=0,\n",
        "        # randomly shift images horizontally\n",
        "        width_shift_range=0.1,\n",
        "        # randomly shift images vertically\n",
        "        height_shift_range=0.1,\n",
        "        # set range for random shear\n",
        "        shear_range=0.,\n",
        "        # set range for random zoom\n",
        "        zoom_range=0.,\n",
        "        # set range for random channel shifts\n",
        "        channel_shift_range=0.,\n",
        "        # set mode for filling points outside the input boundaries\n",
        "        fill_mode='nearest',\n",
        "        # value used for fill_mode = \"constant\"\n",
        "        cval=0.,\n",
        "        # randomly flip images\n",
        "        horizontal_flip=True,\n",
        "        # randomly flip images\n",
        "        vertical_flip=False,\n",
        "        # set rescaling factor (applied before any other transformation)\n",
        "        rescale=None,\n",
        "        # set function that will be applied on each input\n",
        "        preprocessing_function=None,\n",
        "        # image data format, either \"channels_first\" or \"channels_last\"\n",
        "        data_format=None,\n",
        "        # fraction of images reserved for validation (strictly between 0 and 1)\n",
        "        validation_split=0.0)\n",
        "\n",
        "    # Compute quantities required for featurewise normalization\n",
        "    # (std, mean, and principal components if ZCA whitening is applied).\n",
        "    datagen.fit(x_train)\n",
        "\n",
        "    # Fit the model on the batches generated by datagen.flow().\n",
        "    #history = model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),\n",
        "    history = model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),\n",
        "                    validation_data=(x_val, y_val),\n",
        "                    epochs=epochs, verbose=1, workers=4,\n",
        "                    callbacks=callbacks)\n",
        "\n",
        "# Score trained model.\n",
        "scores = model.evaluate(x_test, y_test, verbose=1,batch_size=batch_size,)\n",
        "\n",
        "\n",
        "def plot_model_history(history):\n",
        "  # Plot training loss values\n",
        "  print(\"Plot loss values (Train vs. Validation)\")\n",
        "  plt.figure(figsize=(8, 5))\n",
        "  plt.plot(history.history['loss'],label='Training Loss')\n",
        "  plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "  plt.title('Model loss')\n",
        "  plt.ylabel('Loss')\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.legend()\n",
        "  plt.show()\n",
        "\n",
        "  print(\"Plot the accuracy (Train and Validation)\")\n",
        "  plt.figure(figsize=(8, 5))\n",
        "  plt.plot(history.history['accuracy'],label='Training Accuracy')\n",
        "  plt.plot(history.history['val_accuracy'],label='Validation Accuracy')\n",
        "  plt.title('Model accuracy')\n",
        "  plt.ylabel('Accuracy')\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.legend()\n",
        "  plt.show()\n",
        "\n",
        "plot_model_history(history)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6NwO-up0OWjY"
      },
      "source": [
        "#CBAM with cross validation\n",
        "\n",
        "from __future__ import print_function\n",
        "import keras\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from keras.layers import Dense, Conv2D, BatchNormalization, Activation\n",
        "from keras.layers import AveragePooling2D, Input, Flatten\n",
        "from tensorflow.keras.optimizers import Adam, RMSprop, SGD\n",
        "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.regularizers import l2\n",
        "from keras import backend as K\n",
        "from keras.models import Model\n",
        "from keras.datasets import cifar10\n",
        "import resnet_v1, mobilenets\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import KFold\n",
        "import os\n",
        "\n",
        "# Training parameters\n",
        "batch_size = 128\n",
        "epochs = 80\n",
        "data_augmentation = False\n",
        "num_classes = 7\n",
        "subtract_pixel_mean = False  # Subtracting pixel mean improves accuracy\n",
        "base_model = 'MobileNet'\n",
        "# Choose what attention_module to use: cbam_block / se_block / None\n",
        "\n",
        "\n",
        "# Load dataset.\n",
        "x, y = load_CKplus_full()\n",
        "x, x_test, y, y_test = train_test_split(x, y,test_size=0.1, random_state=2021)\n",
        "\n",
        "#x, y = pre_process(x, y)\n",
        "kfold = KFold(n_splits=4, shuffle=True, random_state=2021)\n",
        "\n",
        "'''# Load the copy of Dataset\n",
        "x_train = trainx.copy()\n",
        "y_train = trainy.copy()\n",
        "x_test = testx.copy()\n",
        "y_test = testy.copy()\n",
        "x_val = valx.copy()\n",
        "y_val = valy.copy()'''\n",
        "\n",
        "# Input image dimensions.\n",
        "input_shape = x_train.shape[1:]\n",
        "\n",
        "# Convert class vectors to binary class matrices.\n",
        "y = to_categorical(y, num_classes)\n",
        "y_test = to_categorical(y_test, num_classes)\n",
        "\n",
        "depth = 20 # For ResNet, specify the depth (e.g. ResNet50: depth=50, ResNet101: Depth=101)\n",
        "model = resnet_v1.resnet_v1(input_shape=input_shape, depth=depth,num_classes=7, attention_module='cbam_block')\n",
        "#model = mobilenets.MobileNet(input_shape=input_shape, classes=num_classes, attention_module='cbam_block')\n",
        "#model.compile(loss='categorical_crossentropy',optimizer=Adam(lr=lr_schedule(0)),metrics=['accuracy'])\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=Adam(), #SGD(decay=2e-5, momentum=0.9, nesterov=True),\n",
        "              metrics=['accuracy'])\n",
        "#model.summary()\n",
        "\n",
        "\n",
        "# Prepare callbacks for model saving and for learning rate adjustment.\n",
        "checkpoint = ModelCheckpoint(filepath=filepath,\n",
        "                             monitor='val_acc',\n",
        "                             verbose=1,\n",
        "                             save_best_only=True)\n",
        "def lr_schedule(epoch):\n",
        "    lr = 0.01\n",
        "    if epoch >= 40:\n",
        "      lr = 0.0001\n",
        "    elif epoch >= 20:\n",
        "      lr=0.001\n",
        "    print('Learning rate: ', lr)\n",
        "    return lr\n",
        "\n",
        "lr_scheduler = LearningRateScheduler(lr_schedule)\n",
        "lr_reducer = ReduceLROnPlateau(factor=np.sqrt(0.1),\n",
        "                               cooldown=0,\n",
        "                               patience=5,\n",
        "                               min_lr=0.5e-6)\n",
        "callbacks = [checkpoint, lr_reducer, lr_scheduler]\n",
        "\n",
        "for train, val in kfold.split(x, y):\n",
        "# Run training, with or without data augmentation.\n",
        "  if not data_augmentation:\n",
        "      print('Not using data augmentation.')\n",
        "      #history = model.fit(x_train, y_train,batch_size=batch_size, epochs=epochs,validation_data=(x_val, y_val), shuffle=True,callbacks=callbacks)\n",
        "      history = model.fit(x[train], y[train],batch_size=batch_size, epochs=epochs,validation_data=(x[val], y[val]),shuffle=True, callbacks=callbacks)\n",
        "  else:\n",
        "      print('Using real-time data augmentation.')\n",
        "      # This will do preprocessing and realtime data augmentation:\n",
        "      datagen = ImageDataGenerator(\n",
        "          # set input mean to 0 over the dataset\n",
        "          featurewise_center=False,\n",
        "          # set each sample mean to 0\n",
        "          samplewise_center=False,\n",
        "          # divide inputs by std of dataset\n",
        "          featurewise_std_normalization=False,\n",
        "          # divide each input by its std\n",
        "          samplewise_std_normalization=False,\n",
        "          # apply ZCA whitening\n",
        "          zca_whitening=False,\n",
        "          # epsilon for ZCA whitening\n",
        "          zca_epsilon=1e-06,\n",
        "          # randomly rotate images in the range (deg 0 to 180)\n",
        "          rotation_range=0,\n",
        "          # randomly shift images horizontally\n",
        "          width_shift_range=0.1,\n",
        "          # randomly shift images vertically\n",
        "          height_shift_range=0.1,\n",
        "          # set range for random shear\n",
        "          shear_range=0.,\n",
        "          # set range for random zoom\n",
        "          zoom_range=0.,\n",
        "          # set range for random channel shifts\n",
        "          channel_shift_range=0.,\n",
        "          # set mode for filling points outside the input boundaries\n",
        "          fill_mode='nearest',\n",
        "          # value used for fill_mode = \"constant\"\n",
        "          cval=0.,\n",
        "          # randomly flip images\n",
        "          horizontal_flip=True,\n",
        "          # randomly flip images\n",
        "          vertical_flip=False,\n",
        "          # set rescaling factor (applied before any other transformation)\n",
        "          rescale=None,\n",
        "          # set function that will be applied on each input\n",
        "          preprocessing_function=None,\n",
        "          # image data format, either \"channels_first\" or \"channels_last\"\n",
        "          data_format=None,\n",
        "          # fraction of images reserved for validation (strictly between 0 and 1)\n",
        "          validation_split=0.0)\n",
        "\n",
        "      # Compute quantities required for featurewise normalization\n",
        "      # (std, mean, and principal components if ZCA whitening is applied).\n",
        "      datagen.fit(x_train)\n",
        "\n",
        "      # Fit the model on the batches generated by datagen.flow().\n",
        "      #history = model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),\n",
        "      history = model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),\n",
        "                      validation_data=(x_val, y_val),\n",
        "                      epochs=epochs, verbose=1, workers=4,\n",
        "                      callbacks=callbacks)\n",
        "\n",
        "# Score trained model.\n",
        "scores = model.evaluate(x_test, y_test, verbose=1,batch_size=batch_size,)\n",
        "\n",
        "\n",
        "def plot_model_history(history):\n",
        "  # Plot training loss values\n",
        "  print(\"Plot loss values (Train vs. Validation)\")\n",
        "  plt.figure(figsize=(8, 5))\n",
        "  plt.plot(history.history['loss'],label='Training Loss')\n",
        "  plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "  plt.title('Model loss')\n",
        "  plt.ylabel('Loss')\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.legend()\n",
        "  plt.show()\n",
        "\n",
        "  print(\"Plot the accuracy (Train and Validation)\")\n",
        "  plt.figure(figsize=(8, 5))\n",
        "  plt.plot(history.history['accuracy'],label='Training Accuracy')\n",
        "  plt.plot(history.history['val_accuracy'],label='Validation Accuracy')\n",
        "  plt.title('Model accuracy')\n",
        "  plt.ylabel('Accuracy')\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.legend()\n",
        "  plt.show()\n",
        "\n",
        "plot_model_history(history)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_EzN95MKPpkU"
      },
      "source": [
        "#test images\n",
        "pred = model.predict(x_test)\n",
        "plot_image(x_test,pred.argmax(1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HbAsFmKTUyxd"
      },
      "source": [
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D\n",
        "from tensorflow.keras.losses import sparse_categorical_crossentropy, categorical_crossentropy\n",
        "from tensorflow.keras.optimizers import Adam, RMSprop\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Model configuration\n",
        "batch_size = 128\n",
        "loss_function = categorical_crossentropy\n",
        "no_classes = 7\n",
        "no_epochs = 30\n",
        "optimizer = RMSprop(lr = 0.01)\n",
        "verbosity = 1\n",
        "\n",
        "\n",
        "# Load FER2013 Dataset\n",
        "x_train = trainx.copy()\n",
        "y_train = trainy.copy()\n",
        "x_test = testx.copy()\n",
        "y_test = testy.copy()\n",
        "x_val = valx.copy()\n",
        "y_val = valy.copy()\n",
        "\n",
        "\n",
        "# Input image dimensions.\n",
        "input_shape = x_train.shape[1:]\n",
        "\n",
        "# Normalize data.\n",
        "#x_train = x_train.astype('float32') / 255\n",
        "#x_test = x_test.astype('float32') / 255\n",
        "#x_val = x_test.astype('float32') / 255\n",
        "\n",
        "\n",
        "# Convert class vectors to binary class matrices.\n",
        "y_train = to_categorical(y_train, num_classes)\n",
        "y_test = to_categorical(y_test, num_classes)\n",
        "y_val = to_categorical(y_val, num_classes)\n",
        "\n",
        "# Create the model\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=input_shape))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Conv2D(256, kernel_size=(3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Conv2D(512, kernel_size=(1, 1), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(1, 1)))\n",
        "model.add(Conv2D(1024, kernel_size=(1, 1), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(1, 1)))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(256, activation='relu'))\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dense(no_classes, activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss=loss_function,\n",
        "              optimizer=optimizer,\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Fit data to model\n",
        "history = model.fit(x_train, y_train,\n",
        "            batch_size=batch_size,\n",
        "            epochs=no_epochs,\n",
        "            validation_data = (x_val,y_val),\n",
        "            verbose=verbosity)\n",
        "\n",
        "# Generate generalization metrics\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "print(f'Test loss: {score[0]} / Test accuracy: {score[1]}')\n",
        "\n",
        "# Visualize history\n",
        "# Plot history: Loss\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Validation loss history')\n",
        "plt.ylabel('Loss value')\n",
        "plt.xlabel('No. epoch')\n",
        "plt.show()\n",
        "\n",
        "# Plot history: Accuracy\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('Validation accuracy history')\n",
        "plt.ylabel('Accuracy value (%)')\n",
        "plt.xlabel('No. epoch')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}